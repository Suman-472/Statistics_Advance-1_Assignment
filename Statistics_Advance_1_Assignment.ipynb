{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V9D86Q30sEH"
      },
      "outputs": [],
      "source": [
        "# Q.1 Explain the properties of the F-distribution.\n",
        "\n",
        "## The F-distribution, also known as the Fisher-Snedecor distribution, is a continuous probability distribution that arises frequently in statistical analysis.\n",
        "   # It is characterized by the following properties:\n",
        "     # 1. Shape:\n",
        "        # The F-distribution is always skewed to the right.\n",
        "        # The exact shape of the distribution depends on the degrees of freedom associated with the numerator and denominator of the F-statistic.\n",
        "        # As the degrees of freedom increase, the F-distribution becomes more symmetrical and approaches a normal distribution.\n",
        "     # 2. Range:\n",
        "        # The F-distribution is defined for all non-negative values.\n",
        "        # The F-statistic can take on any value greater than or equal to zero.\n",
        "     # 3. Degrees of Freedom:\n",
        "        # The F-distribution is parameterized by two degrees of freedom:\n",
        "               # Numerator degrees of freedom (df1): This represents the degrees of freedom associated with the numerator of the F-statistic.\n",
        "               # Denominator degrees of freedom (df2): This represents the degrees of freedom associated with the denominator of the F-statistic.\n",
        "        # Different combinations of degrees of freedom result in different F-distributions.\n",
        "     # 4. Mean and Variance:\n",
        "        # The mean of the F-distribution is:  Mean = df2 / (df2 - 2)\n",
        "        # The variance of the F-distribution is:   Variance = (2 * df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4))\n",
        "     # 5. Applications:\n",
        "        # Analysis of Variance (ANOVA): The F-distribution is used to compare the variances of two or more populations.\n",
        "        # Regression Analysis: The F-distribution is used to test the overall significance of a regression model and to compare the fit of different models.\n",
        "        # Other Hypothesis Testing: The F-distribution is used in various other hypothesis tests, such as testing the equality of variances between two populations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.2 In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "\n",
        "## The F-distribution is prominently used in several statistical tests, especially those involving the comparison of variances.\n",
        "  # Here are the primary types of tests where the F-distribution is employed, along with why itâ€™s appropriate for these tests:\n",
        "     # 1. Analysis of Variance (ANOVA):\n",
        "         # Purpose:\n",
        "             # To determine whether there are statistically significant differences between the means of three or more independent groups.\n",
        "         # Why F-Distribution:\n",
        "             # ANOVA relies on comparing the variance between groups to the variance within groups.\n",
        "             # The F-distribution is suitable because it provides a way to assess the ratio of these variances, taking into account the degrees of freedom for each variance estimate.\n",
        "     # 2. Regression Analysis:\n",
        "         # Purpose:\n",
        "             # To compare different regression models to see if a model explains a significant portion of the variance in the dependent variable.\n",
        "         # Why F-Distribution:\n",
        "             # In regression, the F-test assesses the overall significance of the model by comparing the explained variance (due to the model) to the unexplained variance (due to error).\n",
        "             # The F-distribution helps in testing if the observed ratio of variances is significantly greater than what would be expected by chance.\n",
        "     # 3. Comparing Two Variances:\n",
        "         # Purpose:\n",
        "             # To test if two populations have different variances.\n",
        "         # Why F-Distribution:\n",
        "             # When comparing the variances of two samples, the F-distribution is used because it models the ratio of the two sample variances.\n",
        "             # This test can help determine if the variances are significantly different from each other.\n",
        "     # 4. General Linear Model (GLM) Testing:\n",
        "         # Purpose:\n",
        "             # To test hypotheses about the relationship between dependent and independent variables in models that extend beyond simple regression.\n",
        "         # Why F-Distribution:\n",
        "             # GLM includes ANOVA and regression as special cases and often uses the F-distribution to test the overall model fit and the significance of individual predictors."
      ],
      "metadata": {
        "id": "n0p7s2Qc3mB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.3 What are the key assumptions required for conducting an F-test to compare the variances of two populations?\n",
        "\n",
        "## To conduct an F-test for comparing the variances of two populations, several key assumptions must be met to ensure the validity of the test results.\n",
        "   # Here are the primary assumptions:\n",
        "      # 1. Independence of Samples:\n",
        "           # The two samples being compared should be independent of each other, meaning that the observations in one sample do not influence or\n",
        "           # relate to the observations in the other sample. This is crucial to avoid bias in the test results.\n",
        "      # 2. Normality of Populations:\n",
        "           # Both populations from which the samples are drawn should follow a normal distribution.\n",
        "           # The F-test is sensitive to departures from normality, and even slight deviations can lead to incorrect conclusions.\n",
        "           # This assumption is particularly important when sample sizes are small.\n",
        "           # For larger sample sizes, the test may be more robust, but normality is still preferred.\n",
        "      # 3. Random Sampling:\n",
        "           # Each sample should be randomly selected from its respective population.\n",
        "           # Random sampling ensures that the sample accurately represents the population, which is essential for the results of the F-test to be generalizable.\n",
        "      # 4. Scale of Measurement:\n",
        "           # The data in both samples should be continuous and measured on an interval or ratio scale.\n",
        "           # Categorical data or ordinal data do not meet this criterion, as the concept of variance applies meaningfully only to continuous data.\n",
        "      # 5. Positive Variances:\n",
        "           # Both sample variances must be positive, as the test is designed to work with positive variance estimates.\n",
        "           # Since the F-statistic is the ratio of these variances, a zero or negative variance (which is not possible in practice) would invalidate the test."
      ],
      "metadata": {
        "id": "UVQbl_607VGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.4 What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "## Purpose of ANOVA:\n",
        "      # Analysis of Variance (ANOVA) is a statistical method used to compare the means of three or more groups to determine if there are any statistically significant differences among them.\n",
        "      # It helps in understanding whether the variability in the data can be attributed to the group differences or just random variation.\n",
        "      # ANOVA is commonly used in experimental designs where multiple groups or treatments are compared simultaneously.\n",
        "\n",
        "  # How ANOVA Differs from a t-test:\n",
        "     # 1. Number of Groups Compared:\n",
        "           # ANOVA: Compares the means of three or more groups.\n",
        "           # t-test: Compares the means of two groups only.\n",
        "     # 2. Hypotheses:\n",
        "           # ANOVA:\n",
        "                  # Null Hypothesis: All group means are equal.\n",
        "                  # Alternative Hypothesis: At least one group mean is different.\n",
        "           # t-test:\n",
        "                  # Null Hypothesis: The means of the two groups are equal.\n",
        "                  # Alternative Hypothesis: The means of the two groups are different.\n",
        "     # 3. Type of Variance:\n",
        "           # ANOVA: Analyzes the variance within groups and between groups.\n",
        "           # t-test: Analyzes the variance between two groups.\n",
        "     # 4. Applications:\n",
        "           # ANOVA: Used in experiments with multiple factors or levels, like comparing the effects of different drugs on patient outcomes.\n",
        "           # t-test: Used in simpler experiments where only two conditions or groups are compared, like comparing the test scores of two classes.\n",
        "     # 5. Assumptions:\n",
        "           # Both tests assume the data is normally distributed and the variances are equal (homogeneity of variances).\n",
        "           # ANOVA specifically requires independent samples."
      ],
      "metadata": {
        "id": "nR23bo6cCGDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.5  Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "\n",
        "## When to Use One-Way ANOVA Instead of Multiple t-Tests:\n",
        "        # We have more than two groups and want to compare their means to see if at least one group mean is significantly different from the others.\n",
        "\n",
        "  # Why Use One-Way ANOVA:\n",
        "      # 1. Control Over Type I Error Rate:\n",
        "          # Type I Error:\n",
        "                       # The probability of incorrectly rejecting a true null hypothesis (a false positive).\n",
        "          # Multiple t-Tests:\n",
        "                       # Conducting multiple t-tests increases the risk of Type I errors because each test independently carries a risk of error.\n",
        "                       # For example, if we perform 3 t-tests with a significance level of 0.05, the cumulative Type I error rate is higher than 0.05.\n",
        "          # One-Way ANOVA:\n",
        "               # By performing a single overall test, the risk of Type I error remains at the chosen significance level (e.g., 0.05), maintaining better control over false positives.\n",
        "      # 2. Efficiency:\n",
        "          # Conducting a single ANOVA test is more efficient and less cumbersome than performing multiple t-tests. It simplifies the analysis process and interpretation.\n",
        "      # 3. Insightful Results:\n",
        "          # ANOVA not only tells you if there are significant differences among the groups,\n",
        "           # but it can also be followed by post-hoc tests (if needed) to identify which specific groups differ from each other.\n",
        "\n",
        "      # Example:\n",
        "         # Imagine we want to compare the effectiveness of three different diets (A, B, and C) on weight loss.\n",
        "          # Multiple t-Tests: We'd need to perform three separate tests:\n",
        "              # Diet A vs. Diet B\n",
        "              # Diet A vs. Diet C\n",
        "              # Diet B vs. Diet C\n",
        "          # One-Way ANOVA:\n",
        "                     # We perform a single test to check if there is any significant difference among the three diets.\n",
        "                     # If ANOVA indicates a significant difference, we can then conduct post-hoc tests to pinpoint which diets differ from each other."
      ],
      "metadata": {
        "id": "cBEqyGRBFLXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.6  Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "## In ANOVA, variance is partitioned into two main components: between-group variance and within-group variance.\n",
        "  # This partitioning is essential for calculating the F-statistic, which helps determine whether there are any statistically significant differences between the group means.\n",
        "\n",
        "    # 1. Between-Group Variance (Mean Square Between or MSB):\n",
        "            # Between-group variance represents the variation in the data that can be attributed to differences between the group means.\n",
        "                 # It measures how much the means of each group differ from the overall mean (grand mean) of all groups combined.\n",
        "\n",
        "            # To calculate it, we first compute the sum of squares between (SSB) by summing the squared differences between each group mean and the grand mean,\n",
        "                 # weighted by the number of observations in each group.\n",
        "\n",
        "            # The mean square between (MSB) is obtained by dividing the SSB by the degrees of freedom between groups, which is the number of groups minus one (k-1).\n",
        "\n",
        "                           # MSB = (SSB/(k-1))\n",
        "\n",
        "    # 2. Within-Group Variance (Mean Square Within or MSW):\n",
        "            # Within-group variance represents the variation in the data due to differences within each group,\n",
        "                # or how spread out the values are within each individual group around their group mean.\n",
        "                # This component captures random, individual differences that are not explained by the group membership.\n",
        "\n",
        "            # To calculate it, we compute the sum of squares within (SSW) by summing the squared differences between each observation and its respective group mean.\n",
        "\n",
        "            # The mean square within (MSW) is obtained by dividing the SSW by the degrees of freedom within groups, which is the total number of observations minus the number of groups (N-k).\n",
        "\n",
        "                         # MSW = (SSW/(N-k))\n",
        "\n",
        "    # 3. Calculating the F-Statistic:\n",
        "            # The F-statistic is calculated by taking the ratio of the between-group variance to the within-group variance:\n",
        "\n",
        "                         # F = (MSB/MSW)\n",
        "\n",
        "            # If the between-group variance is large relative to the within-group variance, the F-statistic will be high,\n",
        "                # indicating that the differences in group means are more likely to be statistically significant (i.e., not due to random chance).\n",
        "\n",
        "            # A high F-value suggests that at least one group mean differs significantly from the others, prompting further investigation\n",
        "                # (e.g., with post hoc tests) to identify which groups differ."
      ],
      "metadata": {
        "id": "GRHMRh2GKCjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.7 Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "\n",
        "## Compare the classical (frequentist) approach to ANOVA with the Bayesian approach in terms of handling uncertainty, parameter estimation, and hypothesis testing:\n",
        "    # Classical (Frequentist) Approach to ANOVA:\n",
        "         # 1. Handling Uncertainty:\n",
        "                # Assumes a fixed but unknown population parameter.\n",
        "                # Uses confidence intervals to express uncertainty about the parameter estimates.\n",
        "         # 2. Parameter Estimation:\n",
        "                # Estimates parameters using sample data.\n",
        "                # Relies on the likelihood of the observed data given the parameters.\n",
        "                # Assumes that the data comes from a particular distribution (usually normal).\n",
        "         # 3. Hypothesis Testing:\n",
        "                # Uses null and alternative hypotheses to make decisions.\n",
        "                # Relies on p-values to determine statistical significance.\n",
        "                # If the p-value is less than a pre-specified alpha level (e.g., 0.05), the null hypothesis is rejected in favor of the alternative hypothesis.\n",
        "                # Results are typically framed in terms of rejecting or failing to reject the null hypothesis.\n",
        "\n",
        "    # Bayesian Approach to ANOVA:\n",
        "         # 1. Handling Uncertainty:\n",
        "                # Treats population parameters as random variables with probability distributions.\n",
        "                # Uses prior distributions to incorporate existing knowledge or beliefs about the parameters before observing the data.\n",
        "                # Updates these beliefs with the observed data using Bayes' theorem, resulting in posterior distributions.\n",
        "         # 2. Parameter Estimation:\n",
        "                # Estimates parameters using both the observed data and prior distributions.\n",
        "                # The posterior distribution reflects the updated beliefs about the parameters.\n",
        "                # Provides credible intervals, which are intervals within which the parameters lie with a certain probability (e.g., 95% credible interval).\n",
        "         # 3. Hypothesis Testing:\n",
        "                # Uses posterior probabilities to quantify the strength of evidence in favor of different hypotheses.\n",
        "                # Typically does not rely on p-values.\n",
        "                # Can compare models using metrics like the Bayes factor, which quantifies the evidence for one model over another.\n",
        "                # Results are framed in terms of the probability of hypotheses given the data."
      ],
      "metadata": {
        "id": "AD1DUyQWNfGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.8 Question: You have two sets of data representing the incomes of two different professions:\n",
        "      # V Profession A: [48, 52, 55, 60, 62'\n",
        "      # V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
        "        # incomes are equal. What are your conclusions based on the F-test?\n",
        "      # Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "      # Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison.\n",
        "\n",
        "\n",
        "##\n",
        "\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Data for the two professions\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "f_statistic = np.var(profession_A, ddof=1) / np.var(profession_B, ddof=1)\n",
        "dfn = len(profession_A) - 1  # degrees of freedom for profession A\n",
        "dfd = len(profession_B) - 1  # degrees of freedom for profession B\n",
        "\n",
        "p_value = 2 * min(stats.f.cdf(f_statistic, dfn, dfd), 1 - stats.f.cdf(f_statistic, dfn, dfd))\n",
        "\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "if p_value < 0.05:\n",
        "    print(\"There is a significant difference in variances.\")\n",
        "else:\n",
        "    print(\"There is no significant difference in variances.\")"
      ],
      "metadata": {
        "id": "Q1mHEc5YSlz4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5919621-b894-45df-8619-be3c164b27a8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n",
            "There is no significant difference in variances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q.9 Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "               # average heights between three different regions with the following data:\n",
        "      # V Region A: [160, 162, 165, 158, 164]\n",
        "      # V Region B: [172, 175, 170, 168, 174]\n",
        "      # V Region C: [180, 182, 179, 185, 183]\n",
        "      # V Task: Write Python code to perform the one-way ANOVA and interpret the results.\n",
        "      # V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n",
        "\n",
        "##\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "# Data for the three regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "f_statistic, p_value = f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value > alpha:\n",
        "    print(\"There are no statistically significant differences in average heights between the three regions.\")\n",
        "else:\n",
        "    print(\"There are statistically significant differences in average heights between the three regions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGLwgA_y0Xd1",
        "outputId": "a85316b7-3c6c-4afc-f9d9-402e28137648"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "There are statistically significant differences in average heights between the three regions.\n"
          ]
        }
      ]
    }
  ]
}